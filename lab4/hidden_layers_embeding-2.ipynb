{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T21:05:43.606973Z",
     "start_time": "2020-04-20T21:05:43.605357Z"
    }
   },
   "source": [
    "# Naural network hidden layers activation embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook we will use fully connected neural network for a classification task to improve visualizations algorithm from previous classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fully connected neural network, the output of each layer is computed using the activations from the previous one. In neural network training process, each successive layer learns to extract features from data with increasingly higher levels of abstraction. In this exercise, instead of directly visualizing data, we'll try to visualize the activation of hidden layers in neural networks. Using this idea, we can improve the process of data visualization, and on the other hand, see how processing this data looks like by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first stage, we define simple architecture of the neural network and train it to recognize digits in the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:53:09.426512Z",
     "start_time": "2020-04-20T18:53:09.422227Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_\n",
    "import seaborn as sns\n",
    "import sklearn.datasets\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:33:43.608599Z",
     "start_time": "2020-04-20T18:33:43.606943Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout layers have the very specific function to drop out a random set of activations in that layers by setting them to zero in the forward pass. Simple as that.\n",
    "It allows to avoid overfitting but has to be used only at training time and not at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:54:15.671856Z",
     "start_time": "2020-04-20T18:54:15.670226Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set dropout rate - fractions of neurons to drop\n",
    "\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:56:23.689088Z",
     "start_time": "2020-04-20T18:56:23.551307Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build very simple neural network with 2 hidden layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:56:24.652076Z",
     "start_time": "2020-04-20T18:56:24.476850Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The binary_crossentropy loss expects a one-hot-vector as input,\n",
    "# so we apply the to_categorical function from keras.utilis to convert integer labels to one-hot-vectors.\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:56:25.191318Z",
     "start_time": "2020-04-20T18:56:25.189236Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:56:26.057660Z",
     "start_time": "2020-04-20T18:56:25.986968Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "# Put everything on grayscale\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:56:28.288303Z",
     "start_time": "2020-04-20T18:56:28.250869Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split training and validation data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, train_size=5/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:56:28.944869Z",
     "start_time": "2020-04-20T18:56:28.859422Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show example digit\n",
    "plt.imshow(X_train[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:57:16.684930Z",
     "start_time": "2020-04-20T18:56:29.382615Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When we have defined and compiled the model, it can be trained using the fit function.\n",
    "# We also use validation dataset to monitor validation loss and accuracy.\n",
    "\n",
    "network_history = model.fit(X_train, Y_train, batch_size=128, \n",
    "                            epochs=20, verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:59:51.203228Z",
     "start_time": "2020-04-20T18:59:51.199854Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(network_history.history['accuracy'])\n",
    "    plt.plot(network_history.history['val_accuracy'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:59:52.184750Z",
     "start_time": "2020-04-20T18:59:52.028617Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit function return keras.callbacks.History object which contains the entire history\n",
    "# of training/validation loss, accuracy and other metrices for each epoch.\n",
    "# We can therefore plot the behaviour of loss and accuracy during the training phase.\n",
    "\n",
    "plot_history(network_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:59:57.382554Z",
     "start_time": "2020-04-20T18:59:57.379602Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keras Model have summary function, that print data about model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T19:00:24.342199Z",
     "start_time": "2020-04-20T19:00:24.228437Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We are interested in downloading the activation of hidden layers, because the dropout layers are between them,\n",
    "# we need to properly select the index of the three dense layers.\n",
    "get_layer_output = K.function([model.layers[0].input],\n",
    "                              [model.layers[0].output, model.layers[2].output, model.layers[4].output])\n",
    "\n",
    "layer1_output, layer2_output, layer3_output = get_layer_output([X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T19:00:25.191221Z",
     "start_time": "2020-04-20T19:00:25.187505Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ids = [np.arange(len(Y_train))[Y_train[:,i] == 1] for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2 graphs below are not directly related to the topic of the exercise, but they visualize very well how neuron activation actives work and for explanation are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T19:00:27.216699Z",
     "start_time": "2020-04-20T19:00:27.089486Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "# this animation shows what the example number 5 looks like\n",
    "# and what activations of neurons look in hidden layers of the neural network\n",
    "\n",
    "\n",
    "\n",
    "# digit to be plotted\n",
    "digit = 5\n",
    "\n",
    "# indices of frames to be plotted for this digit\n",
    "n = range(50)\n",
    "\n",
    "# initialize plots\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,4))\n",
    "\n",
    "# prepare plots\n",
    "ax1.set_title('Input Layer', fontsize=16)\n",
    "ax1.axes.get_xaxis().set_visible(False)\n",
    "ax1.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax2.set_title('Hidden Layer 1', fontsize=16)\n",
    "ax2.axes.get_xaxis().set_visible(False)\n",
    "ax2.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax3.set_title('Hidden Layer 2', fontsize=16)\n",
    "ax3.axes.get_xaxis().set_visible(False)\n",
    "ax3.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "ax4.set_title('Output Layer', fontsize=16)\n",
    "ax4.axes.get_xaxis().set_visible(False)\n",
    "ax4.axes.get_yaxis().set_visible(False)   \n",
    "\n",
    "# add numbers to the output layer plot to indicate label\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        text = ax4.text(j, i, [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, '', '']][i][j],\n",
    "                        ha=\"center\", va=\"center\", color=\"w\", fontsize=16)    \n",
    "        \n",
    "def animate(id):\n",
    "    # plot elements that are changed in the animation\n",
    "    digit_plot = ax1.imshow(X_train[train_ids[digit][id]].reshape((28,28)), animated=True)\n",
    "    layer1_plot = ax2.imshow(layer1_output[train_ids[digit][id]].reshape((16,16)), animated=True)\n",
    "    layer2_plot = ax3.imshow(layer2_output[train_ids[digit][id]].reshape((8,8)), animated=True)\n",
    "    output_plot = ax4.imshow(np.append(layer3_output[train_ids[digit][id]], \n",
    "                                       [np.nan, np.nan]).reshape((3,4)), animated=True)\n",
    "    return digit_plot, layer1_plot, layer2_plot, output_plot,\n",
    "\n",
    "# define animation\n",
    "ani = matplotlib.animation.FuncAnimation(f, animate, frames=n, interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T19:00:49.767737Z",
     "start_time": "2020-04-20T19:00:30.192866Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases the same subset of neurons fires, while other neurons remain quiescent. This is much more obvious in the second hidden layer than in the first hidden layer and can be interpreted as the first layer pre-processesing the pixel data, while the second layer deals with pattern recognition.\n",
    "\n",
    "This effect is mainly caused by regularization forced by dropout. Dropout generally leads to the sparse weight matrices where a significant part of connection weights are close to 0. Insignificant weights are suppressed.\n",
    "\n",
    "\n",
    "Optional, nonobligatory task:\n",
    "You can easily see how the visualizations change if you comment lines responsible for the dropout \"model.add(Dropout(dropout))\".\n",
    "Remember to change \"get_layer_output\", because after removing the dropout, the dense layers will have indexes: 0,1,2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T19:01:12.350351Z",
     "start_time": "2020-04-20T19:01:12.222021Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Let's check the similarity in behavior for frames showing the same digit by looking at the ensemble properties.\n",
    "# In this case, ensemble properties refers to how the neurons behave on average\n",
    "# for a large number of frames showing the same digit.\n",
    "\n",
    "# digit to be plotted\n",
    "digit = 6\n",
    "\n",
    "# numbers of frames to be summed over\n",
    "n = np.append([1], np.linspace(5, 100, 20, dtype=int))\n",
    "\n",
    "# initialize plots\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,4))\n",
    "\n",
    "# add a counter indicating the number of frames used in the summation\n",
    "counter = ax1.text(1, 2, 'n={}'.format(0), color='white', fontsize=16, animated=True)\n",
    "\n",
    "# prepare plots\n",
    "ax1.set_title('Input Layer', fontsize=16)\n",
    "ax1.axes.get_xaxis().set_visible(False)\n",
    "ax1.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax2.set_title('Hidden Layer 1', fontsize=16)\n",
    "ax2.axes.get_xaxis().set_visible(False)\n",
    "ax2.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax3.set_title('Hidden Layer 2', fontsize=16)\n",
    "ax3.axes.get_xaxis().set_visible(False)\n",
    "ax3.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "ax4.set_title('Output Layer', fontsize=16)\n",
    "ax4.axes.get_xaxis().set_visible(False)\n",
    "ax4.axes.get_yaxis().set_visible(False)   \n",
    "\n",
    "# add numbers to the output layer plot to indicate label\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        text = ax4.text(j, i, [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, '', '']][i][j],\n",
    "                        ha=\"center\", va=\"center\", color=\"w\", fontsize=16)    \n",
    "        \n",
    "def animate(id):\n",
    "    # plot elements that are changed in the animation\n",
    "    digit_plot = ax1.imshow(np.sum(X_train[train_ids[digit][:id]], axis=0).reshape((28,28)), animated=True)\n",
    "    layer1_plot = ax2.imshow(np.sum(layer1_output[train_ids[digit][:id]], axis=0).reshape((16,16)), animated=True)\n",
    "    layer2_plot = ax3.imshow(np.sum(layer2_output[train_ids[digit][:id]], axis=0).reshape((8,8)), animated=True)\n",
    "    output_plot = ax4.imshow(np.append(np.sum(layer3_output[train_ids[digit][:id]], axis=0), \n",
    "                                       [np.nan, np.nan]).reshape((3,4)), animated=True)\n",
    "    counter.set_text('n={}'.format(id))\n",
    "    return digit_plot, layer1_plot, layer2_plot, output_plot, counter,\n",
    "\n",
    "# define animation\n",
    "ani = matplotlib.animation.FuncAnimation(f, animate, frames=n, interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T19:01:26.980616Z",
     "start_time": "2020-04-20T19:01:22.753833Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After summing up the responses of as little as 20-30 frames, the pattern in the second hidden layer is almost static. After combining about 70-80 frames, also the pattern in the first hidden layer appears static. This supports the idea that only a subset of all neurons is involved in the recognition of individual digits.\n",
    "\n",
    "Especially the above plot is important when we think about use of neural networks for data visualization. We can clearly see that the activation generated by examples belonging to the same class are less chaotic than the examples themselves, therefore their visualization should give a more clustered structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "- project a mnist training part into 2-dimensional space using t-SNE and UMAP.\n",
    "\n",
    "- use layer1_output and layer2_output to project first and second hidden layers of neural network into a 2-dimensional space. Also divided into a test and training set, use the same methods as the point above.\n",
    "\n",
    "- also visualize the test part.\n",
    "\n",
    "- Try to use 2-dimensional projection for classification task.\n",
    "\n",
    "- Use embeddings lerned on raw train data (and also on hidden activations of train data) to transform test data (and also hidden activations of test data) into 2-dimensional space.\n",
    "\n",
    "- Use the k-nearest neighbors algorithm to classify transformed points from the test set. Use the KNN algorithm in which you will use points from the training set as a neighbor with known class assignment. Becouse t-SNE is a non-linear, non-parametric embedding you cant use already learned t-SNE to transform new points into the existing embedded space. So for this part, use only UMAP with have fit_transform method (learn manifold) and also transform (only project new data to existing manifold). Try with few values of n_neighbors e.g [3, 5, 10]\n",
    "\n",
    "- Estimate the accuracy of classification using this approach. Use all 3 layers (raw data, 1 hidden layer, 2 hidden layer) and few values of n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja części treningowej zbioru MNIST z wykorzystaniem metody TSNE i UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = sklearn.datasets.fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_mtest, y_train, y_mtest = train_test_split(mnist.data, mnist['target'], train_size=5/6, random_state=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = TSNE(n_components = 2, random_state=0)\n",
    "tsne_mnist = model.fit_transform(X_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.scatterplot(x = tsne_mnist[:,0], y = tsne_mnist[:,1], hue = y_train, \n",
    "                palette = sns.hls_palette(10), legend = 'full');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = umap.UMAP(n_components = 2, random_state=42)\n",
    "umap_mnist = model.fit_transform(X_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.scatterplot(x = umap_mnist[:,0], y = umap_mnist[:,1], hue = y_train, \n",
    "                palette = sns.hls_palette(10), legend = 'full');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja części treningowej pierwszej warstwy ukrytej dla TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_mtest, y_train, y_mtest = train_test_split(layer1_output, Y_train, train_size=5/6, random_state=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.argmax(y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(n_components = 2, random_state=0)\n",
    "tsne_mnist = model.fit_transform(X_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.scatterplot(x = tsne_mnist[:,0], y = tsne_mnist[:,1], hue = y_train, \n",
    "                palette = sns.hls_palette(10), legend = 'full');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja części testowej pierwszej warstwy ukrytej dla TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mtest = np.argmax(y_mtest, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(n_components = 2, random_state=0)\n",
    "tsne_mnist = model.fit_transform(X_mtest)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.scatterplot(x = tsne_mnist[:,0], y = tsne_mnist[:,1], hue = y_mtest, \n",
    "                palette = sns.hls_palette(10), legend = 'full');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja części treningowej pierwszej warstwy ukrytej dla UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = umap.UMAP(n_components = 2, random_state=42)\n",
    "umap_mnist = model.fit_transform(X_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.scatterplot(x = umap_mnist[:,0], y = umap_mnist[:,1], hue = y_train, \n",
    "                palette = sns.hls_palette(10), legend = 'full');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja części testowej pierwszej warstwy ukrytej dla UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = umap.UMAP(n_components = 2, random_state=42)\n",
    "umap_mnist = model.fit_transform(X_mtest)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.scatterplot(x = umap_mnist[:,0], y = umap_mnist[:,1], hue = y_mtest, \n",
    "                palette = sns.hls_palette(10), legend = 'full');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja części treningowej drugiej warstwy ukrytej dla TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_mtest, y_train, y_mtest = train_test_split(layer2_output, Y_train, train_size=5/6, random_state=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.argmax(y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(n_components = 2, random_state=0)\n",
    "tsne_mnist = model.fit_transform(X_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.scatterplot(x = tsne_mnist[:,0], y = tsne_mnist[:,1], hue = y_train, \n",
    "                palette = sns.hls_palette(10), legend = 'full');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja części testowej drugiej warstwy ukrytej dla TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mtest = np.argmax(y_mtest, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(n_components = 2, random_state=0)\n",
    "tsne_mnist = model.fit_transform(X_mtest)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.scatterplot(x = tsne_mnist[:,0], y = tsne_mnist[:,1], hue = y_mtest, \n",
    "                palette = sns.hls_palette(10), legend = 'full');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja części treningowej drugiej warstwy ukrytej dla UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = umap.UMAP(n_components = 2, random_state=42)\n",
    "umap_mnist = model.fit_transform(X_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.scatterplot(x = umap_mnist[:,0], y = umap_mnist[:,1], hue = y_train, \n",
    "                palette = sns.hls_palette(10), legend = 'full');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja części testowej drugiej warstwy ukrytej dla UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = umap.UMAP(n_components = 2, random_state=42)\n",
    "umap_mnist = model.fit_transform(X_mtest)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.scatterplot(x = umap_mnist[:,0], y = umap_mnist[:,1], hue = y_mtest, \n",
    "                palette = sns.hls_palette(10), legend = 'full');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
